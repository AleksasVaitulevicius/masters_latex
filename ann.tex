\subsubsection{Dirbtinis neuronas, perceptronas}

Perceptronas -  tai iteratyviai apmokomas tiesinis klasifikatorius, kuris susideda iš $\{x_{0}, x_{1}, x_{2}, ..., x_{n}\}$ mokymo aibės vektorių, vadinamais įėjimais, $\{w_{0}, w_{1}, w_{2}, ..., w_{n}\} \in \R$ perdavimo koeficientų, vadinamų svoriais, aktyvacijos (perdavimo) funkcijos $f(a)$ ir $\{y_{0}, y_{1}, y_{2}, ..., y_{n}\}$ reikšmių, vadinamų išėjimais. Įėjimas $x_{0}$ yra vadinamas nuliniu įėjimu ir jo reikšmė yra pastovi $x_{0} = 1$, o $w_{0}$ - nuliniu svoriu arba slenksčiu (angl. bias). Aktyvacijos funkcijos argumentas yra įėjimo reikšmių ir svorių sandaugų suma:

\begin{equation}
	a = \sum_{k = 1}^{n} w_{k}x_{k}
\end{equation}

Dažniausiai yra naudojamos šios aktyvacijos funkcijos: slenkstinė (angl. unit step) \ref{eqn:unitStep}, sigmoidinė (angl. sigmoid) \ref{eqn:sigmoid}, gabalais tiesinė (angl. piecewise linear) \ref{eqn:pieceLinear}, Gauso (angl. Gaussian) \ref{eqn:gaussian} ir tiesinė (angl. linear) \ref{eqn:linear}

\begin{equation}
\label{eqn:unitStep}
	f(a) =
	\begin{cases}
		0, & \mbox{if } \beta > a \\
		1, & \mbox{if } \beta \leq a
	\end{cases}
\end{equation}

\begin{equation}
	\label{eqn:sigmoid}
	f(a) = \dfrac{1}{1 + \exp^{-\beta a}}
\end{equation}

\begin{equation}
	\label{eqn:pieceLinear}
	f(a) =
	\begin{cases}
		0, & \mbox{if } a_{min} \geq a \\
		ma + b, & \mbox{if } a_{min} < a < a_{max} \\
		1, & \mbox{if } a_{max} \leq a
	\end{cases}
\end{equation}

\begin{equation}
	\label{eqn:gaussian}
	f(a) = \dfrac{1}{\sqrt{2\pi\sigma}} \exp^{\dfrac{-(x - \mu)^2}{2\sigma^2}}
\end{equation}

\begin{equation}
	\label{eqn:linear}
	f(a) = ma + b
\end{equation}

Perceptrono mokymas yra iteratyvus procesas, kuriame randami svoriai $W = \{w_{0}, w_{1}, w_{2}, ..., w_{n}\}$, su kuriais funkcijos \ref{eqn:mse} rezultatas įgyja mažiausią reikšmę. Funkcijoje \ref{eqn:mse} $y_i$ yra perceptrono i-tasis išėjimas ir $t_i$ - i-tojo įėjimo norima klasė.

% coming up next: rasyk kad generuoja random pirminius svorius, tada minimizavimo funkcija

\begin{equation}
\label{eqn:mse}
e(w) = \dfrac{1}{n}\sum_{i=1}^{n}(y_i - t_i)^2
\end{equation}

\subsubsection{Dirbtiniai neuroniniai tinklai}